{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c0d77e4",
   "metadata": {},
   "source": [
    "## Semantic Segmentation \n",
    "- FCN with a ResNet backbone is a classic, reliable baseline that balances clarity and performance.\n",
    "- Cross-Entropy with `ignore_index=255` is standard for VOC where unlabeled pixels are marked as 255.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "405ab6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Setup & Imports\n",
    "# ===============================\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# Reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "log_dir = 'runs/segmentation_experiment'\n",
    "checkpoint_path = 'checkpoint.pth'\n",
    "num_workers = 2 if torch.cuda.is_available() else 0\n",
    "print(f\"Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d0ed137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'img_size': (256, 256), 'batch_size': 8, 'num_classes': 21, 'learning_rate': 0.001, 'ignore_index': 255, 'use_pretrained_backbone': False, 'checkpoint_last': 'checkpoint.pth', 'checkpoint_best': 'checkpoint_best.pth'}\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Configuration\n",
    "# ===============================\n",
    "config = {\n",
    "    'img_size': (256, 256),\n",
    "    'batch_size': 8,\n",
    "    'num_classes': 21,\n",
    "    'learning_rate': 1e-3,\n",
    "    'ignore_index': 255,\n",
    "    'use_pretrained_backbone': False,\n",
    "    'checkpoint_last': 'checkpoint.pth',\n",
    "    'checkpoint_best': 'checkpoint_best.pth',\n",
    "}\n",
    "print(config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb947001",
   "metadata": {},
   "source": [
    "### Data and Augmentations\n",
    "\n",
    "We use Pascal VOC2012 already available in your workspace (`VOCdevkit/VOC2012`).\n",
    "\n",
    "- Images are resized for consistent batching.\n",
    "- Light augmentations are applied for regularization.\n",
    "- Masks use nearest-neighbor interpolation to preserve class indices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7748a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 183, Val batches: 182\n"
     ]
    }
   ],
   "source": [
    "class ToTensorMask:\n",
    "    def __call__(self, sample):\n",
    "        image, target = sample  # image: CHW uint8, target: HW uint8\n",
    "        image = image.float() / 255.0\n",
    "        return image, target.long()\n",
    "\n",
    "class ResizePair:\n",
    "    def __init__(self, size: Tuple[int, int]):\n",
    "        self.size = size\n",
    "    def __call__(self, sample):\n",
    "        image, target = sample\n",
    "        image = torch.nn.functional.interpolate(image.unsqueeze(0), size=self.size, mode='bilinear', align_corners=False).squeeze(0)\n",
    "        target = torch.nn.functional.interpolate(target.unsqueeze(0).unsqueeze(0).float(), size=self.size, mode='nearest').squeeze(0).squeeze(0).long()\n",
    "        return image, target\n",
    "\n",
    "class RandomHorizontalFlipPair:\n",
    "    def __init__(self, p: float = 0.5):\n",
    "        self.p = p\n",
    "    def __call__(self, sample):\n",
    "        image, target = sample\n",
    "        if random.random() < self.p:\n",
    "            image = torch.flip(image, dims=[2])  # flip width\n",
    "            target = torch.flip(target, dims=[1])\n",
    "        return image, target\n",
    "\n",
    "class ColorJitterOnlyImage:\n",
    "    def __init__(self):\n",
    "        self.tf = T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05)\n",
    "    def __call__(self, sample):\n",
    "        image, target = sample\n",
    "        # Convert to PIL for color jitter then back to tensor [0,1]\n",
    "        img_pil = T.ToPILImage()(image)\n",
    "        img_pil = self.tf(img_pil)\n",
    "        image = T.ToTensor()(img_pil)\n",
    "        return image, target\n",
    "\n",
    "class ComposePair:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "    def __call__(self, sample):\n",
    "        for t in self.transforms:\n",
    "            sample = t(sample)\n",
    "        return sample\n",
    "\n",
    "img_size = (256, 256)\n",
    "\n",
    "train_joint_tf = ComposePair([\n",
    "    ResizePair(img_size),\n",
    "    RandomHorizontalFlipPair(p=0.5),\n",
    "    ColorJitterOnlyImage(),\n",
    "    ToTensorMask(),\n",
    "])\n",
    "\n",
    "val_joint_tf = ComposePair([\n",
    "    ResizePair(img_size),\n",
    "    ToTensorMask(),\n",
    "])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = torch.stack([b[0] for b in batch], dim=0)\n",
    "    targets = torch.stack([b[1] for b in batch], dim=0)\n",
    "    return images, targets\n",
    "\n",
    "class VOCPaired(VOCSegmentation):\n",
    "    def __init__(self, root, year='2012', image_set='train', download=False, joint_transform=None):\n",
    "        super().__init__(root=root, year=year, image_set=image_set, download=download)\n",
    "        self.joint_transform = joint_transform\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.images[index]\n",
    "        mask_path = self.masks[index]\n",
    "        # Read tensors directly (no PIL): img CHW uint8, target HW uint8\n",
    "        img = torchvision.io.read_image(img_path)\n",
    "        target = torchvision.io.read_image(mask_path)[0]\n",
    "        if self.joint_transform is not None:\n",
    "            img, target = self.joint_transform((img, target))\n",
    "        return img, target\n",
    "\n",
    "project_root = os.path.abspath('.')  # e.g., /Users/ortalhanuna/my-code\n",
    "voc_expected = os.path.join(project_root, 'VOCdevkit', 'VOC2012')\n",
    "if not os.path.isdir(voc_expected):\n",
    "    print(f\"VOC2012 not found at {voc_expected}. Will attempt download to {project_root}.\")\n",
    "    download_flag = True\n",
    "else:\n",
    "    download_flag = False\n",
    "\n",
    "batch_size = 8\n",
    "train_ds = VOCPaired(root=project_root, year='2012', image_set='train', download=download_flag, joint_transform=train_joint_tf)\n",
    "val_ds = VOCPaired(root=project_root, year='2012', image_set='val', download=False, joint_transform=val_joint_tf)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d4043c",
   "metadata": {},
   "source": [
    "### Class Balance Visualization\n",
    "\n",
    "We compute class frequency from the mask labels (ignoring label 255). This helps understand dataset bias and informs augmentation or loss re-weighting if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdae8502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_class_histogram(dataloader, num_classes: int = 21) -> np.ndarray:\n",
    "    hist = np.zeros(num_classes, dtype=np.int64)\n",
    "    for images, targets in dataloader:\n",
    "        targets_np = targets.numpy()\n",
    "        mask = targets_np != 255\n",
    "        valid = targets_np[mask]\n",
    "        binc = np.bincount(valid.flatten(), minlength=num_classes)\n",
    "        hist[:len(binc)] += binc\n",
    "    return hist\n",
    "\n",
    "def plot_class_histogram(hist: np.ndarray, class_names=None):\n",
    "    classes = np.arange(len(hist))\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.bar(classes, hist, color='skyblue')\n",
    "    plt.xlabel('Class Index')\n",
    "    plt.ylabel('Pixel Count')\n",
    "    plt.title('Class Frequency (ignore=255 not shown)')\n",
    "    if class_names is not None and len(class_names)==len(hist):\n",
    "        plt.xticks(classes, class_names, rotation=90)\n",
    "    else:\n",
    "        plt.xticks(classes)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compute on a subset for speed\n",
    "subset_batches = min(50, len(train_loader))\n",
    "hist = np.zeros(21, dtype=np.int64)\n",
    "for i, (_, targets) in enumerate(train_loader):\n",
    "    if i >= subset_batches:\n",
    "        break\n",
    "    targets_np = targets.numpy()\n",
    "    mask = targets_np != 255\n",
    "    valid = targets_np[mask]\n",
    "    binc = np.bincount(valid.flatten(), minlength=21)\n",
    "    hist += binc\n",
    "\n",
    "voc_class_names = [\n",
    "    'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat',\n",
    "    'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant',\n",
    "    'sheep', 'sofa', 'train', 'tvmonitor'\n",
    "]\n",
    "\n",
    "assert len(voc_class_names) == 21\n",
    "\n",
    "plot_class_histogram(hist, class_names=voc_class_names)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb2ef5",
   "metadata": {},
   "source": [
    "### Model: Simple FCN with ResNet18 Backbone\n",
    "\n",
    "Rationale:\n",
    "- FCN-style decoder is simple and interpretable.\n",
    "- ResNet18 provides a lightweight yet expressive encoder.\n",
    "- We upsample logits back to input resolution using bilinear interpolation (no transposed convolutions).\n",
    "- The decoder is a small head: 3x3 conv (512256), ReLU, then 1x1 conv to class logits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6294cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model params: 12.36M\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "class SimpleFCN(nn.Module):\n",
    "    def __init__(self, num_classes: int = 21):\n",
    "        super().__init__()\n",
    "        backbone = models.resnet18(weights=None)\n",
    "        self.encoder = nn.Sequential(\n",
    "            backbone.conv1,\n",
    "            backbone.bn1,\n",
    "            backbone.relu,\n",
    "            backbone.maxpool,\n",
    "            backbone.layer1,\n",
    "            backbone.layer2,\n",
    "            backbone.layer3,\n",
    "            backbone.layer4,\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, num_classes, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[-2:]\n",
    "        feats = self.encoder(x)\n",
    "        logits = self.classifier(feats)\n",
    "        logits = torch.nn.functional.interpolate(logits, size=(h, w), mode='bilinear', align_corners=False)\n",
    "        return logits\n",
    "\n",
    "num_classes = 21\n",
    "model = SimpleFCN(num_classes=num_classes).to(device)\n",
    "print(f\"Model params: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37156a6",
   "metadata": {},
   "source": [
    "### Loss, Optimizer, and Checkpointing\n",
    "\n",
    "Choice of loss:\n",
    "- We use `CrossEntropyLoss(ignore_index=255)` which is standard for per-pixel classification with VOC masks.\n",
    "- Ignore index ensures unlabeled pixels do not affect gradients.\n",
    "\n",
    "Optimizer:\n",
    "- Adam with a moderate learning rate is a robust default for baselines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dd1706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "\n",
    "def compute_class_weights(loader, num_classes=21, ignore_index=255):\n",
    "    counts = torch.zeros(num_classes)\n",
    "    for _, m in loader:\n",
    "        m = m.view(-1)\n",
    "        m = m[(m>=0) & (m<num_classes)]\n",
    "        counts += torch.bincount(m, minlength=num_classes).float()\n",
    "    freq = counts / counts.sum().clamp_min(1)\n",
    "    weights = 1.0 / (freq + 1e-6)\n",
    "    weights = (weights / weights.mean()).clamp(max=10.0)\n",
    "    return weights\n",
    "\n",
    "class_weights = compute_class_weights(train_loader).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=255, weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, path=checkpoint_path):\n",
    "    torch.save({\n",
    "        'model_state': model.state_dict(),\n",
    "        'optimizer_state': optimizer.state_dict(),\n",
    "        'epoch': epoch\n",
    "    }, path)\n",
    "    print(f\"Checkpoint saved at epoch {epoch}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer=None, path=checkpoint_path):\n",
    "    if os.path.exists(path):\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        model.load_state_dict(checkpoint.get('model_state', checkpoint), strict=False)\n",
    "        if optimizer is not None and 'optimizer_state' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "        start_epoch = int(checkpoint.get('epoch', 0)) + 1\n",
    "        print(f\"Checkpoint loaded. Resuming from epoch {start_epoch}\")\n",
    "        return start_epoch\n",
    "    return 0\n",
    "\n",
    "start_epoch = load_checkpoint(model, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6886034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits_from_model(model, images):\n",
    "    out = model(images)\n",
    "    if isinstance(out, dict):\n",
    "        return out['out']\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db927a54",
   "metadata": {},
   "source": [
    "### Validation, Inference and Metrics\n",
    "\n",
    "We track pixel accuracy and mean IoU (mIoU). Predictions are `argmax` over class logits. TensorBoard logs include scalar metrics and example images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4a3c575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pixel_accuracy(preds: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "    mask = targets != 255\n",
    "    correct = (preds[mask] == targets[mask]).sum().item()\n",
    "    total = mask.sum().item()\n",
    "    return correct / max(total, 1)\n",
    "\n",
    "\n",
    "def compute_confusion_matrix(preds: torch.Tensor, targets: torch.Tensor, num_classes: int) -> np.ndarray:\n",
    "    mask = targets != 255\n",
    "    preds = preds[mask].view(-1).cpu().numpy()\n",
    "    targets = targets[mask].view(-1).cpu().numpy()\n",
    "    cm = np.bincount(num_classes * targets + preds, minlength=num_classes**2)\n",
    "    return cm.reshape(num_classes, num_classes)\n",
    "\n",
    "\n",
    "def compute_mIoU_from_cm(conf_matrix: np.ndarray) -> float:\n",
    "    intersection = np.diag(conf_matrix)\n",
    "    ground_truth_set = conf_matrix.sum(axis=1)\n",
    "    predicted_set = conf_matrix.sum(axis=0)\n",
    "    union = ground_truth_set + predicted_set - intersection\n",
    "    iou = intersection / np.maximum(union, 1)\n",
    "    valid = ground_truth_set > 0\n",
    "    return float(np.mean(iou[valid]))\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, num_classes=21):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    conf_matrix = np.zeros((num_classes, num_classes), dtype=np.int64)\n",
    "    with torch.no_grad():\n",
    "        for images, targets in loader:\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            logits = get_logits_from_model(model, images)\n",
    "            loss = criterion(logits, targets)\n",
    "            val_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            conf_matrix += compute_confusion_matrix(preds, targets, num_classes)\n",
    "    val_loss /= max(len(loader), 1)\n",
    "    miou = compute_mIoU_from_cm(conf_matrix)\n",
    "    pixel_acc = np.diag(conf_matrix).sum() / max(conf_matrix.sum(), 1)\n",
    "    return val_loss, pixel_acc, miou\n",
    "\n",
    "\n",
    "def inference(model, images):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        logits = get_logits_from_model(model, images)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "    return preds, logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a98e2cc",
   "metadata": {},
   "source": [
    "### Visualization: Image vs. Ground Truth vs. Prediction\n",
    "\n",
    "We overlay predictions and ground truth masks for sanity checks. Colors are mapped per class index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a102c372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple color map for 21 classes\n",
    "voc_colors = np.array([\n",
    "    [0, 0, 0],        # background\n",
    "    [128, 0, 0], [0, 128, 0], [128, 128, 0], [0, 0, 128],\n",
    "    [128, 0, 128], [0, 128, 128], [128, 128, 128], [64, 0, 0],\n",
    "    [192, 0, 0], [64, 128, 0], [192, 128, 0], [64, 0, 128],\n",
    "    [192, 0, 128], [64, 128, 128], [192, 128, 128], [0, 64, 0],\n",
    "    [128, 64, 0], [0, 192, 0], [128, 192, 0], [0, 64, 128]\n",
    "], dtype=np.uint8)\n",
    "\n",
    "\n",
    "def label_to_color(mask: np.ndarray) -> np.ndarray:\n",
    "    h, w = mask.shape\n",
    "    color = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    for cls in range(min(21, len(voc_colors))):\n",
    "        color[mask == cls] = voc_colors[cls]\n",
    "    return color\n",
    "\n",
    "\n",
    "def plot_batch_predictions(images: torch.Tensor, targets: torch.Tensor, preds: torch.Tensor, max_items: int = 4):\n",
    "    images = images.cpu()\n",
    "    targets = targets.cpu().numpy()\n",
    "    preds = preds.cpu().numpy()\n",
    "    n = min(max_items, images.size(0))\n",
    "    plt.figure(figsize=(12, 3*n))\n",
    "    for i in range(n):\n",
    "        img = images[i].permute(1,2,0).numpy()\n",
    "        gt_color = label_to_color(targets[i])\n",
    "        pr_color = label_to_color(preds[i])\n",
    "\n",
    "        plt.subplot(n, 3, 3*i+1); plt.imshow(img); plt.axis('off'); plt.title('Image')\n",
    "        plt.subplot(n, 3, 3*i+2); plt.imshow(gt_color); plt.axis('off'); plt.title('Ground Truth')\n",
    "        plt.subplot(n, 3, 3*i+3); plt.imshow(pr_color); plt.axis('off'); plt.title('Prediction')\n",
    "    plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6953c876",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "We log per-epoch loss, pixel accuracy, and mIoU. We also save checkpoints and push example predictions to TensorBoard for qualitative tracking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f3fb4a",
   "metadata": {},
   "source": [
    "### DeepLabV3-ResNet50 (pretrained) comparison\n",
    "\n",
    "We evaluate a strong off-the-shelf baseline from `torchvision.models.segmentation` and compare to our SimpleFCN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d45004f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DeepLabV3-ResNet50': {'val_loss': 3.0355, 'pixel_acc': np.float64(0.0794), 'mIoU': 0.0105}}\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as tv_models\n",
    "\n",
    "\n",
    "def build_deeplabv3_resnet50(num_classes: int = 21, pretrained: bool = True):\n",
    "    # Load pretrained on COCO-stuff/COCO (21 classes here will require replacing the classifier)\n",
    "    weights = tv_models.segmentation.DeepLabV3_ResNet50_Weights.DEFAULT if pretrained else None\n",
    "    model_dl = tv_models.segmentation.deeplabv3_resnet50(weights=weights)\n",
    "    # Replace classifier head to match our number of classes\n",
    "    in_channels = model_dl.classifier[-1].in_channels\n",
    "    model_dl.classifier[-1] = nn.Conv2d(in_channels, num_classes, kernel_size=1)\n",
    "    return model_dl\n",
    "\n",
    "\n",
    "def validate_deeplab(model_dl, loader, criterion, num_classes: int = 21):\n",
    "    model_dl.eval()\n",
    "    val_loss = 0.0\n",
    "    conf_matrix = np.zeros((num_classes, num_classes), dtype=np.int64)\n",
    "    with torch.no_grad():\n",
    "        for images, targets in loader:\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            out = model_dl(images)\n",
    "            logits = out['out']  # DeepLab outputs dict\n",
    "            loss = criterion(logits, targets)\n",
    "            val_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            conf_matrix += compute_confusion_matrix(preds, targets, num_classes)\n",
    "    val_loss /= max(len(loader), 1)\n",
    "    miou = compute_mIoU_from_cm(conf_matrix)\n",
    "    pixel_acc = np.diag(conf_matrix).sum() / max(conf_matrix.sum(), 1)\n",
    "    return val_loss, pixel_acc, miou\n",
    "\n",
    "\n",
    "# Build and evaluate DeepLabV3-ResNet50\n",
    "model_deeplab = build_deeplabv3_resnet50(num_classes=num_classes, pretrained=True).to(device)\n",
    "deeplab_val_loss, deeplab_pix_acc, deeplab_miou = validate_deeplab(model_deeplab, val_loader, criterion, num_classes=num_classes)\n",
    "print({\n",
    "    'DeepLabV3-ResNet50': {\n",
    "        'val_loss': round(deeplab_val_loss, 4),\n",
    "        'pixel_acc': round(deeplab_pix_acc, 4),\n",
    "        'mIoU': round(deeplab_miou, 4)\n",
    "    }\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "281bbffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>pixel_acc</th>\n",
       "      <th>mIoU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SimpleFCN-ResNet18</td>\n",
       "      <td>3.0700</td>\n",
       "      <td>0.7102</td>\n",
       "      <td>0.0343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DeepLabV3-ResNet50</td>\n",
       "      <td>3.0355</td>\n",
       "      <td>0.0794</td>\n",
       "      <td>0.0105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model  val_loss  pixel_acc    mIoU\n",
       "0  SimpleFCN-ResNet18    3.0700     0.7102  0.0343\n",
       "1  DeepLabV3-ResNet50    3.0355     0.0794  0.0105"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Always (re)compute SimpleFCN validation to make comparison fair\n",
    "val_loss_fcn, pix_acc_fcn, miou_fcn = validate(model, val_loader, criterion, num_classes=num_classes)\n",
    "\n",
    "# Ensure DeepLab metrics exist; if not, build and evaluate on the fly\n",
    "need_eval_deeplab = True\n",
    "try:\n",
    "    _ = (deeplab_val_loss, deeplab_pix_acc, deeplab_miou)\n",
    "    need_eval_deeplab = False\n",
    "except NameError:\n",
    "    need_eval_deeplab = True\n",
    "\n",
    "if need_eval_deeplab:\n",
    "    import torchvision.models as tv_models\n",
    "\n",
    "    def build_deeplabv3_resnet50(num_classes: int = 21, pretrained: bool = True):\n",
    "        weights = tv_models.segmentation.DeepLabV3_ResNet50_Weights.DEFAULT if pretrained else None\n",
    "        model_dl = tv_models.segmentation.deeplabv3_resnet50(weights=weights)\n",
    "        in_channels = model_dl.classifier[-1].in_channels\n",
    "        model_dl.classifier[-1] = nn.Conv2d(in_channels, num_classes, kernel_size=1)\n",
    "        return model_dl\n",
    "\n",
    "    def validate_deeplab(model_dl, loader, criterion, num_classes: int = 21):\n",
    "        model_dl.eval()\n",
    "        val_loss = 0.0\n",
    "        conf_matrix = np.zeros((num_classes, num_classes), dtype=np.int64)\n",
    "        with torch.no_grad():\n",
    "            for images, targets in loader:\n",
    "                images = images.to(device)\n",
    "                targets = targets.to(device)\n",
    "                out = model_dl(images)\n",
    "                logits = out['out']\n",
    "                loss = criterion(logits, targets)\n",
    "                val_loss += loss.item()\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                conf_matrix += compute_confusion_matrix(preds, targets, num_classes)\n",
    "        val_loss /= max(len(loader), 1)\n",
    "        miou = compute_mIoU_from_cm(conf_matrix)\n",
    "        pixel_acc = np.diag(conf_matrix).sum() / max(conf_matrix.sum(), 1)\n",
    "        return val_loss, pixel_acc, miou\n",
    "\n",
    "    model_deeplab = build_deeplabv3_resnet50(num_classes=num_classes, pretrained=True).to(device)\n",
    "    deeplab_val_loss, deeplab_pix_acc, deeplab_miou = validate_deeplab(model_deeplab, val_loader, criterion, num_classes=num_classes)\n",
    "\n",
    "comparison = [\n",
    "    {\n",
    "        'model': 'SimpleFCN-ResNet18',\n",
    "        'val_loss': round(val_loss_fcn, 4),\n",
    "        'pixel_acc': round(pix_acc_fcn, 4),\n",
    "        'mIoU': round(miou_fcn, 4),\n",
    "    },\n",
    "    {\n",
    "        'model': 'DeepLabV3-ResNet50',\n",
    "        'val_loss': round(deeplab_val_loss, 4),\n",
    "        'pixel_acc': round(deeplab_pix_acc, 4),\n",
    "        'mIoU': round(deeplab_miou, 4),\n",
    "    }\n",
    "]\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame(comparison)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d00a4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 tensor(3.0658, grad_fn=<NllLoss2DBackward0>)\n",
      "0 1 tensor(3.0884, grad_fn=<NllLoss2DBackward0>)\n",
      "0 2 tensor(2.9760, grad_fn=<NllLoss2DBackward0>)\n",
      "0 3 tensor(3.0163, grad_fn=<NllLoss2DBackward0>)\n",
      "0 4 tensor(2.8944, grad_fn=<NllLoss2DBackward0>)\n",
      "0 5 tensor(3.0707, grad_fn=<NllLoss2DBackward0>)\n",
      "0 6 tensor(3.0971, grad_fn=<NllLoss2DBackward0>)\n",
      "0 7 tensor(2.9406, grad_fn=<NllLoss2DBackward0>)\n",
      "0 8 tensor(3.1304, grad_fn=<NllLoss2DBackward0>)\n",
      "0 9 tensor(3.3189, grad_fn=<NllLoss2DBackward0>)\n",
      "0 10 tensor(3.0019, grad_fn=<NllLoss2DBackward0>)\n",
      "0 11 tensor(2.9679, grad_fn=<NllLoss2DBackward0>)\n",
      "0 12 tensor(3.2118, grad_fn=<NllLoss2DBackward0>)\n",
      "0 13 tensor(2.8286, grad_fn=<NllLoss2DBackward0>)\n",
      "0 14 tensor(3.0198, grad_fn=<NllLoss2DBackward0>)\n",
      "0 15 tensor(2.9108, grad_fn=<NllLoss2DBackward0>)\n",
      "0 16 tensor(3.0277, grad_fn=<NllLoss2DBackward0>)\n",
      "0 17 tensor(3.2889, grad_fn=<NllLoss2DBackward0>)\n",
      "0 18 tensor(3.0950, grad_fn=<NllLoss2DBackward0>)\n",
      "0 19 tensor(3.1078, grad_fn=<NllLoss2DBackward0>)\n",
      "0 20 tensor(2.9468, grad_fn=<NllLoss2DBackward0>)\n",
      "0 21 tensor(2.8691, grad_fn=<NllLoss2DBackward0>)\n",
      "0 22 tensor(3.0442, grad_fn=<NllLoss2DBackward0>)\n",
      "0 23 tensor(3.0939, grad_fn=<NllLoss2DBackward0>)\n",
      "0 24 tensor(2.9687, grad_fn=<NllLoss2DBackward0>)\n",
      "0 25 tensor(3.0116, grad_fn=<NllLoss2DBackward0>)\n",
      "0 26 tensor(2.8366, grad_fn=<NllLoss2DBackward0>)\n",
      "0 27 tensor(3.0194, grad_fn=<NllLoss2DBackward0>)\n",
      "0 28 tensor(3.1165, grad_fn=<NllLoss2DBackward0>)\n",
      "0 29 tensor(2.8711, grad_fn=<NllLoss2DBackward0>)\n",
      "0 30 tensor(2.9826, grad_fn=<NllLoss2DBackward0>)\n",
      "0 31 tensor(3.2628, grad_fn=<NllLoss2DBackward0>)\n",
      "0 32 tensor(2.7248, grad_fn=<NllLoss2DBackward0>)\n",
      "0 33 tensor(2.8136, grad_fn=<NllLoss2DBackward0>)\n",
      "0 34 tensor(3.1564, grad_fn=<NllLoss2DBackward0>)\n",
      "0 35 tensor(2.8331, grad_fn=<NllLoss2DBackward0>)\n",
      "0 36 tensor(3.3570, grad_fn=<NllLoss2DBackward0>)\n",
      "0 37 tensor(3.0632, grad_fn=<NllLoss2DBackward0>)\n",
      "0 38 tensor(3.2076, grad_fn=<NllLoss2DBackward0>)\n",
      "0 39 tensor(3.3212, grad_fn=<NllLoss2DBackward0>)\n",
      "0 40 tensor(3.0646, grad_fn=<NllLoss2DBackward0>)\n",
      "0 41 tensor(2.9725, grad_fn=<NllLoss2DBackward0>)\n",
      "0 42 tensor(2.9367, grad_fn=<NllLoss2DBackward0>)\n",
      "0 43 tensor(3.1299, grad_fn=<NllLoss2DBackward0>)\n",
      "0 44 tensor(2.8362, grad_fn=<NllLoss2DBackward0>)\n",
      "0 45 tensor(3.1116, grad_fn=<NllLoss2DBackward0>)\n",
      "0 46 tensor(3.1869, grad_fn=<NllLoss2DBackward0>)\n",
      "0 47 tensor(3.0580, grad_fn=<NllLoss2DBackward0>)\n",
      "0 48 tensor(2.9214, grad_fn=<NllLoss2DBackward0>)\n",
      "0 49 tensor(3.2676, grad_fn=<NllLoss2DBackward0>)\n",
      "0 50 tensor(3.1145, grad_fn=<NllLoss2DBackward0>)\n",
      "0 51 tensor(3.2511, grad_fn=<NllLoss2DBackward0>)\n",
      "0 52 tensor(3.2217, grad_fn=<NllLoss2DBackward0>)\n",
      "0 53 tensor(3.2576, grad_fn=<NllLoss2DBackward0>)\n",
      "0 54 tensor(3.1154, grad_fn=<NllLoss2DBackward0>)\n",
      "0 55 tensor(3.0429, grad_fn=<NllLoss2DBackward0>)\n",
      "0 56 tensor(3.0688, grad_fn=<NllLoss2DBackward0>)\n",
      "0 57 tensor(3.0236, grad_fn=<NllLoss2DBackward0>)\n",
      "0 58 tensor(2.9811, grad_fn=<NllLoss2DBackward0>)\n",
      "0 59 tensor(3.0846, grad_fn=<NllLoss2DBackward0>)\n",
      "0 60 tensor(3.1045, grad_fn=<NllLoss2DBackward0>)\n",
      "0 61 tensor(2.9490, grad_fn=<NllLoss2DBackward0>)\n",
      "0 62 tensor(3.0560, grad_fn=<NllLoss2DBackward0>)\n",
      "0 63 tensor(3.0251, grad_fn=<NllLoss2DBackward0>)\n",
      "0 64 tensor(3.0537, grad_fn=<NllLoss2DBackward0>)\n",
      "0 65 tensor(3.1268, grad_fn=<NllLoss2DBackward0>)\n",
      "0 66 tensor(2.9756, grad_fn=<NllLoss2DBackward0>)\n",
      "0 67 tensor(3.0139, grad_fn=<NllLoss2DBackward0>)\n",
      "0 68 tensor(2.9858, grad_fn=<NllLoss2DBackward0>)\n",
      "0 69 tensor(2.8219, grad_fn=<NllLoss2DBackward0>)\n",
      "0 70 tensor(3.0092, grad_fn=<NllLoss2DBackward0>)\n",
      "0 71 tensor(3.1528, grad_fn=<NllLoss2DBackward0>)\n",
      "0 72 tensor(3.1277, grad_fn=<NllLoss2DBackward0>)\n",
      "0 73 tensor(2.9720, grad_fn=<NllLoss2DBackward0>)\n",
      "0 74 tensor(3.2319, grad_fn=<NllLoss2DBackward0>)\n",
      "0 75 tensor(2.9715, grad_fn=<NllLoss2DBackward0>)\n",
      "0 76 tensor(3.0699, grad_fn=<NllLoss2DBackward0>)\n",
      "0 77 tensor(3.1706, grad_fn=<NllLoss2DBackward0>)\n",
      "0 78 tensor(3.1703, grad_fn=<NllLoss2DBackward0>)\n",
      "0 79 tensor(3.0077, grad_fn=<NllLoss2DBackward0>)\n",
      "0 80 tensor(3.1593, grad_fn=<NllLoss2DBackward0>)\n",
      "0 81 tensor(2.8890, grad_fn=<NllLoss2DBackward0>)\n",
      "0 82 tensor(2.9651, grad_fn=<NllLoss2DBackward0>)\n",
      "0 83 tensor(3.1281, grad_fn=<NllLoss2DBackward0>)\n",
      "0 84 tensor(2.9914, grad_fn=<NllLoss2DBackward0>)\n",
      "0 85 tensor(2.8756, grad_fn=<NllLoss2DBackward0>)\n",
      "0 86 tensor(3.1194, grad_fn=<NllLoss2DBackward0>)\n",
      "0 87 tensor(3.0969, grad_fn=<NllLoss2DBackward0>)\n",
      "0 88 tensor(3.0804, grad_fn=<NllLoss2DBackward0>)\n",
      "0 89 tensor(2.9147, grad_fn=<NllLoss2DBackward0>)\n",
      "0 90 tensor(2.9095, grad_fn=<NllLoss2DBackward0>)\n",
      "0 91 tensor(2.9133, grad_fn=<NllLoss2DBackward0>)\n",
      "0 92 tensor(3.2112, grad_fn=<NllLoss2DBackward0>)\n",
      "0 93 tensor(3.1957, grad_fn=<NllLoss2DBackward0>)\n",
      "0 94 tensor(3.1319, grad_fn=<NllLoss2DBackward0>)\n",
      "0 95 tensor(3.0413, grad_fn=<NllLoss2DBackward0>)\n",
      "0 96 tensor(2.9520, grad_fn=<NllLoss2DBackward0>)\n",
      "0 97 tensor(2.9883, grad_fn=<NllLoss2DBackward0>)\n",
      "0 98 tensor(2.9860, grad_fn=<NllLoss2DBackward0>)\n",
      "0 99 tensor(3.0956, grad_fn=<NllLoss2DBackward0>)\n",
      "0 100 tensor(2.9047, grad_fn=<NllLoss2DBackward0>)\n",
      "0 101 tensor(2.7880, grad_fn=<NllLoss2DBackward0>)\n",
      "0 102 tensor(2.9821, grad_fn=<NllLoss2DBackward0>)\n",
      "0 103 tensor(3.1878, grad_fn=<NllLoss2DBackward0>)\n",
      "0 104 tensor(3.0258, grad_fn=<NllLoss2DBackward0>)\n",
      "0 105 tensor(3.2055, grad_fn=<NllLoss2DBackward0>)\n",
      "0 106 tensor(2.8846, grad_fn=<NllLoss2DBackward0>)\n",
      "0 107 tensor(2.9767, grad_fn=<NllLoss2DBackward0>)\n",
      "0 108 tensor(2.9191, grad_fn=<NllLoss2DBackward0>)\n",
      "0 109 tensor(3.0443, grad_fn=<NllLoss2DBackward0>)\n",
      "0 110 tensor(3.2769, grad_fn=<NllLoss2DBackward0>)\n",
      "0 111 tensor(3.1601, grad_fn=<NllLoss2DBackward0>)\n",
      "0 112 tensor(3.2293, grad_fn=<NllLoss2DBackward0>)\n",
      "0 113 tensor(3.0599, grad_fn=<NllLoss2DBackward0>)\n",
      "0 114 tensor(3.0490, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/threading.py\"\u001b[0m, line \u001b[35m1041\u001b[0m, in \u001b[35m_bootstrap_inner\u001b[0m\n",
      "    \u001b[31mself.run\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/ortalhanuna/my-code/.venv/lib/python3.13/site-packages/tensorboard/summary/writer/event_file_writer.py\"\u001b[0m, line \u001b[35m244\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    \u001b[31mself._run\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/ortalhanuna/my-code/.venv/lib/python3.13/site-packages/tensorboard/summary/writer/event_file_writer.py\"\u001b[0m, line \u001b[35m275\u001b[0m, in \u001b[35m_run\u001b[0m\n",
      "    \u001b[31mself._record_writer.write\u001b[0m\u001b[1;31m(data)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/ortalhanuna/my-code/.venv/lib/python3.13/site-packages/tensorboard/summary/writer/record_writer.py\"\u001b[0m, line \u001b[35m40\u001b[0m, in \u001b[35mwrite\u001b[0m\n",
      "    \u001b[31mself._writer.write\u001b[0m\u001b[1;31m(header + header_crc + data + footer_crc)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/ortalhanuna/my-code/.venv/lib/python3.13/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py\"\u001b[0m, line \u001b[35m775\u001b[0m, in \u001b[35mwrite\u001b[0m\n",
      "    \u001b[31mself.fs.append\u001b[0m\u001b[1;31m(self.filename, file_content, self.binary_mode)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/ortalhanuna/my-code/.venv/lib/python3.13/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py\"\u001b[0m, line \u001b[35m167\u001b[0m, in \u001b[35mappend\u001b[0m\n",
      "    \u001b[31mself._write\u001b[0m\u001b[1;31m(filename, file_content, \"ab\" if binary_mode else \"a\")\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/ortalhanuna/my-code/.venv/lib/python3.13/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py\"\u001b[0m, line \u001b[35m171\u001b[0m, in \u001b[35m_write\u001b[0m\n",
      "    with \u001b[31mio.open\u001b[0m\u001b[1;31m(filename, mode, encoding=encoding)\u001b[0m as f:\n",
      "         \u001b[31m~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "\u001b[1;35mFileNotFoundError\u001b[0m: \u001b[35m[Errno 2] No such file or directory: b'runs/segmentation_experiment/events.out.tfevents.1757278246.ip-10-0-0-6.eu-west-1.compute.internal.2267.0'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 115 tensor(2.8484, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: b'runs/segmentation_experiment/events.out.tfevents.1757278246.ip-10-0-0-6.eu-west-1.compute.internal.2267.0'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     26\u001b[39m     running_loss += loss.item()\n\u001b[32m     27\u001b[39m     global_step = epoch * \u001b[38;5;28mlen\u001b[39m(train_loader) + step\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_scalar\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain/batch_loss\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28mprint\u001b[39m(epoch ,step , loss)\n\u001b[32m     31\u001b[39m train_loss = running_loss / \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_loader), \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/my-code/.venv/lib/python3.13/site-packages/torch/utils/tensorboard/writer.py:381\u001b[39m, in \u001b[36mSummaryWriter.add_scalar\u001b[39m\u001b[34m(self, tag, scalar_value, global_step, walltime, new_style, double_precision)\u001b[39m\n\u001b[32m    376\u001b[39m torch._C._log_api_usage_once(\u001b[33m\"\u001b[39m\u001b[33mtensorboard.logging.add_scalar\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    378\u001b[39m summary = scalar(\n\u001b[32m    379\u001b[39m     tag, scalar_value, new_style=new_style, double_precision=double_precision\n\u001b[32m    380\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_file_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwalltime\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/my-code/.venv/lib/python3.13/site-packages/torch/utils/tensorboard/writer.py:115\u001b[39m, in \u001b[36mFileWriter.add_summary\u001b[39m\u001b[34m(self, summary, global_step, walltime)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Add a `Summary` protocol buffer to the event file.\u001b[39;00m\n\u001b[32m    103\u001b[39m \n\u001b[32m    104\u001b[39m \u001b[33;03mThis method wraps the provided summary in an `Event` protocol buffer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m    walltime (from time.time()) seconds after epoch\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    114\u001b[39m event = event_pb2.Event(summary=summary)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwalltime\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/my-code/.venv/lib/python3.13/site-packages/torch/utils/tensorboard/writer.py:99\u001b[39m, in \u001b[36mFileWriter.add_event\u001b[39m\u001b[34m(self, event, step, walltime)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     96\u001b[39m     \u001b[38;5;66;03m# Make sure step is converted from numpy or other formats\u001b[39;00m\n\u001b[32m     97\u001b[39m     \u001b[38;5;66;03m# since protobuf might not convert depending on version\u001b[39;00m\n\u001b[32m     98\u001b[39m     event.step = \u001b[38;5;28mint\u001b[39m(step)\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevent_writer\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/my-code/.venv/lib/python3.13/site-packages/tensorboard/summary/writer/event_file_writer.py:117\u001b[39m, in \u001b[36mEventFileWriter.add_event\u001b[39m\u001b[34m(self, event)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, event_pb2.Event):\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    114\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mExpected an event_pb2.Event proto, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    115\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(event)\n\u001b[32m    116\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_async_writer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSerializeToString\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/my-code/.venv/lib/python3.13/site-packages/tensorboard/summary/writer/event_file_writer.py:171\u001b[39m, in \u001b[36m_AsyncWriter.write\u001b[39m\u001b[34m(self, bytestring)\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Enqueue the given bytes to be written asychronously.\"\"\"\u001b[39;00m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Status of the worker should be checked under the lock to avoid\u001b[39;00m\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# multiple threads passing the check and then switching just before\u001b[39;00m\n\u001b[32m    170\u001b[39m     \u001b[38;5;66;03m# blocking on putting to the queue which might result in a deadlock.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_worker_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._closed:\n\u001b[32m    173\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mWriter is closed\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/my-code/.venv/lib/python3.13/site-packages/tensorboard/summary/writer/event_file_writer.py:212\u001b[39m, in \u001b[36m_AsyncWriter._check_worker_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    210\u001b[39m exception = \u001b[38;5;28mself\u001b[39m._worker.exception\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/threading.py:1041\u001b[39m, in \u001b[36mThread._bootstrap_inner\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1038\u001b[39m     _sys.setprofile(_profile_hook)\n\u001b[32m   1040\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1041\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m   1043\u001b[39m     \u001b[38;5;28mself\u001b[39m._invoke_excepthook(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/my-code/.venv/lib/python3.13/site-packages/tensorboard/summary/writer/event_file_writer.py:244\u001b[39m, in \u001b[36m_AsyncWriterThread.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m    246\u001b[39m         \u001b[38;5;28mself\u001b[39m.exception = ex\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/my-code/.venv/lib/python3.13/site-packages/tensorboard/summary/writer/event_file_writer.py:275\u001b[39m, in \u001b[36m_AsyncWriterThread._run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    273\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown_signal:\n\u001b[32m    274\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_record_writer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m     \u001b[38;5;28mself\u001b[39m._has_pending_data = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m queue.Empty:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/my-code/.venv/lib/python3.13/site-packages/tensorboard/summary/writer/record_writer.py:40\u001b[39m, in \u001b[36mRecordWriter.write\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m     38\u001b[39m header_crc = struct.pack(\u001b[33m\"\u001b[39m\u001b[33m<I\u001b[39m\u001b[33m\"\u001b[39m, masked_crc32c(header))\n\u001b[32m     39\u001b[39m footer_crc = struct.pack(\u001b[33m\"\u001b[39m\u001b[33m<I\u001b[39m\u001b[33m\"\u001b[39m, masked_crc32c(data))\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_writer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader_crc\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mfooter_crc\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/my-code/.venv/lib/python3.13/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py:775\u001b[39m, in \u001b[36mGFile.write\u001b[39m\u001b[34m(self, file_content)\u001b[39m\n\u001b[32m    771\u001b[39m         \u001b[38;5;28mself\u001b[39m.write_started = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    773\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    774\u001b[39m         \u001b[38;5;66;03m# append the later chunks\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m775\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbinary_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    777\u001b[39m     \u001b[38;5;66;03m# add to temp file, but wait for flush to write to final filesystem\u001b[39;00m\n\u001b[32m    778\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.write_temp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/my-code/.venv/lib/python3.13/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py:167\u001b[39m, in \u001b[36mLocalFileSystem.append\u001b[39m\u001b[34m(self, filename, file_content, binary_mode)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mappend\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename, file_content, binary_mode=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    160\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Append string file contents to a file.\u001b[39;00m\n\u001b[32m    161\u001b[39m \n\u001b[32m    162\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    165\u001b[39m \u001b[33;03m        binary_mode: bool, write as binary if True, otherwise text\u001b[39;00m\n\u001b[32m    166\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mab\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbinary_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ma\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/my-code/.venv/lib/python3.13/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py:171\u001b[39m, in \u001b[36mLocalFileSystem._write\u001b[39m\u001b[34m(self, filename, file_content, mode)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_write\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename, file_content, mode):\n\u001b[32m    170\u001b[39m     encoding = \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mutf8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    172\u001b[39m         compatify = compat.as_bytes \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;28;01melse\u001b[39;00m compat.as_text\n\u001b[32m    173\u001b[39m         f.write(compatify(file_content))\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: b'runs/segmentation_experiment/events.out.tfevents.1757278246.ip-10-0-0-6.eu-west-1.compute.internal.2267.0'"
     ]
    }
   ],
   "source": [
    "# Training Loop with Visual Output\n",
    "import matplotlib.pyplot as plt\n",
    "model = model_deeplab\n",
    "\n",
    "num_epochs = 30\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_pixel_accs = []\n",
    "val_mious = []\n",
    "\n",
    "start_epoch = 0\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for step, (images, targets) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = get_logits_from_model(model, images)\n",
    "        loss = criterion(logits, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        global_step = epoch * len(train_loader) + step\n",
    "        writer.add_scalar('train/batch_loss', loss.item(), global_step)\n",
    "        print(epoch ,step , loss)\n",
    "\n",
    "    train_loss = running_loss / max(len(train_loader), 1)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    val_loss, val_pix_acc, val_miou = validate(model, val_loader, criterion, num_classes=num_classes)\n",
    "    val_losses.append(val_loss)\n",
    "    val_pixel_accs.append(val_pix_acc)\n",
    "    val_mious.append(val_miou)\n",
    "\n",
    "    writer.add_scalar('train/epoch_loss', train_loss, epoch)\n",
    "    writer.add_scalar('val/loss', val_loss, epoch)\n",
    "    writer.add_scalar('val/pixel_accuracy', val_pix_acc, epoch)\n",
    "    writer.add_scalar('val/mIoU', val_miou, epoch)\n",
    "\n",
    "    # Log example images every epoch\n",
    "    images, targets = next(iter(val_loader))\n",
    "    preds, _ = inference(model, images)\n",
    "    grid = make_grid(images[:8].cpu(), nrow=4, normalize=True)\n",
    "    writer.add_image('val/images', grid, epoch)\n",
    "    writer.add_text('val/sample_metrics', f\"pix_acc={val_pix_acc:.4f}, mIoU={val_miou:.4f}\", epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch:03d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | pix_acc={val_pix_acc:.4f} | mIoU={val_miou:.4f}\")\n",
    "\n",
    "    # Show predictions in Jupyter every 5 epochs\n",
    "    if epoch % 5 == 0 or epoch == num_epochs - 1:\n",
    "        print(f\"\\n=== Epoch {epoch} Predictions ===\")\n",
    "        plot_batch_predictions(images[:4], targets[:4], preds[:4], max_items=4)\n",
    "        \n",
    "        # Plot loss curves\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(val_losses, label='Val Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss Curves')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(val_pixel_accs, label='Pixel Accuracy', color='green')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Pixel Accuracy')\n",
    "        plt.title('Validation Pixel Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(val_mious, label='mIoU', color='red')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('mIoU')\n",
    "        plt.title('Validation mIoU')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    save_checkpoint(model, optimizer, epoch, checkpoint_path)\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "print(\"Training completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
