{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9165bbfe",
   "metadata": {},
   "source": [
    "\n",
    "# Vision Playground — Multi-task PyTorch Notebook\n",
    "\n",
    "This notebook contains modular examples for **multiple computer-vision tasks**, each in its own section so you can run them separately:\n",
    "- Classification: ResNet50 (CIFAR10) and MobileNetV2 (CIFAR100)\n",
    "- Object Detection: Faster R-CNN (ResNet50-FPN)\n",
    "- Instance Segmentation: Mask R-CNN (ResNet50-FPN)\n",
    "- Semantic Segmentation: DeepLabV3 (ResNet50)\n",
    "- Keypoint Detection: Keypoint R-CNN (ResNet50-FPN)\n",
    "- Super-Resolution: SRCNN (simple example)\n",
    "- Generative Models: DCGAN (MNIST example)\n",
    "\n",
    "**Notes before running heavy cells**\n",
    "- Some datasets (COCO) are large and not auto-downloaded here. For detection/segmentation you can test on small local images or use lightweight datasets (PennFudan, VOC) included in torchvision.\n",
    "- Training large models requires a GPU. Many training cells are provided as demonstrative 'one-epoch' or 'smoke-test' runs and are commented for convenience.\n",
    "- The notebook is modular: run only the sections you need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9863288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Utilities: data loaders, training loop, checkpoints, plotting, tensorboard\n",
    "import os, torch, torch.nn as nn, torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "def imshow_batch(images, titles=None, n=6):\n",
    "    fig, axes = plt.subplots(1, n, figsize=(15,3))\n",
    "    for i in range(n):\n",
    "        img = images[i].cpu().permute(1,2,0) * 0.5 + 0.5\n",
    "        axes[i].imshow(img)\n",
    "        if titles: axes[i].set_title(titles[i])\n",
    "        axes[i].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, path):\n",
    "    torch.save({\n",
    "        'model_state': model.state_dict(),\n",
    "        'optim_state': optimizer.state_dict(),\n",
    "        'epoch': epoch\n",
    "    }, path)\n",
    "    print(f\"Checkpoint saved at {path}\")\n",
    "\n",
    "def load_checkpoint(model, optimizer, path):\n",
    "    if os.path.exists(path):\n",
    "        ckpt = torch.load(path)\n",
    "        model.load_state_dict(ckpt['model_state'])\n",
    "        if optimizer is not None:\n",
    "            optimizer.load_state_dict(ckpt['optim_state'])\n",
    "        print(f\"Loaded checkpoint from {path}, epoch {ckpt.get('epoch')}\")\n",
    "        return ckpt.get('epoch', 0)\n",
    "    else:\n",
    "        print(f\"No checkpoint found at {path}\")\n",
    "        return 0\n",
    "\n",
    "# Simple train/eval helpers for classification tasks\n",
    "def train_one_epoch_classification(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for x,y in tqdm(loader, leave=False):\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        _, preds = out.max(1)\n",
    "        correct += preds.eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return running_loss/len(loader.dataset), correct/total\n",
    "\n",
    "def evaluate_classification(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            running_loss += loss.item() * x.size(0)\n",
    "            _, preds = out.max(1)\n",
    "            correct += preds.eq(y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return running_loss/len(loader.dataset), correct/total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcd1ed0",
   "metadata": {},
   "source": [
    "\n",
    "## Classification\n",
    "\n",
    "Two examples:\n",
    "- ResNet50 on CIFAR10 (demo)\n",
    "- MobileNetV2 on CIFAR100 (demo)\n",
    "\n",
    "These are small, runnable examples using torchvision datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d833d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Classification examples: CIFAR10 + CIFAR100 demos\n",
    "from torchvision import models\n",
    "\n",
    "def get_classification_loaders(dataset='CIFAR10', batch_size=128, augment=True):\n",
    "    if dataset == 'CIFAR10':\n",
    "        mean, std = (0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)\n",
    "        train_tfms = T.Compose([T.RandomHorizontalFlip(), T.RandomCrop(32, padding=4), T.ToTensor(), T.Normalize(mean, std)]) if augment else T.Compose([T.ToTensor(), T.Normalize(mean,std)])\n",
    "        test_tfms = T.Compose([T.ToTensor(), T.Normalize(mean,std)])\n",
    "        train_set = torchvision.datasets.CIFAR10('./data', train=True, download=True, transform=train_tfms)\n",
    "        test_set = torchvision.datasets.CIFAR10('./data', train=False, download=True, transform=test_tfms)\n",
    "    elif dataset == 'CIFAR100':\n",
    "        mean, std = (0.5071, 0.4865, 0.4409), (0.2673, 0.2564, 0.2762)\n",
    "        train_tfms = T.Compose([T.RandomHorizontalFlip(), T.RandomCrop(32, padding=4), T.ToTensor(), T.Normalize(mean, std)]) if augment else T.Compose([T.ToTensor(), T.Normalize(mean,std)])\n",
    "        test_tfms = T.Compose([T.ToTensor(), T.Normalize(mean,std)])\n",
    "        train_set = torchvision.datasets.CIFAR100('./data', train=True, download=True, transform=train_tfms)\n",
    "        test_set = torchvision.datasets.CIFAR100('./data', train=False, download=True, transform=test_tfms)\n",
    "    else:\n",
    "        raise ValueError('dataset not supported')\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    return train_loader, test_loader, train_set.classes, train_set\n",
    "\n",
    "# ResNet50 on CIFAR10 (demo)\n",
    "train_loader, test_loader, classes, train_set = get_classification_loaders('CIFAR10', batch_size=128)\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, len(classes))\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Smoke test: run one training epoch (uncomment to run)\n",
    "train_one_epoch_classification(model, train_loader, optimizer, criterion, device)\n",
    "\n",
    "# Visualize a batch\n",
    "images, labels = next(iter(train_loader))\n",
    "imshow_batch(images, [classes[l] for l in labels], n=6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad85d257",
   "metadata": {},
   "source": [
    "\n",
    "## Object Detection — Faster R-CNN (ResNet50-FPN)\n",
    "\n",
    "Uses `torchvision.models.detection.fasterrcnn_resnet50_fpn`. For demo purposes you can use a small custom dataset or PennFudan dataset (included in torchvision tutorials).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba6a819",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Detection model creation + inference smoke test (no full training here)\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "# create model, set number of classes (including background)\n",
    "num_classes = 2  # example: background + 1 object class\n",
    "model_det = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "# For fine-tuning with different number of classes you'd replace the box predictor\n",
    "# but for inference demo we'll use the pretrained model as-is.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_det = model_det.to(device)\n",
    "model_det.eval()\n",
    "\n",
    "# Run inference on a batch of CIFAR images resized to expected sizes as quick demo\n",
    "from torchvision.transforms.functional import resize\n",
    "batch = [T.ToPILImage()(images[i]) for i in range(4)]\n",
    "inputs = [T.ToTensor()(resize(img, (224,224))).to(device) for img in batch]\n",
    "with torch.no_grad():\n",
    "    preds = model_det(inputs)\n",
    "for i,p in enumerate(preds):\n",
    "    print(f'Image {i}: {len(p[\"boxes\"])} boxes (showing scores):', p.get('scores')[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfe981c",
   "metadata": {},
   "source": [
    "\n",
    "## Instance Segmentation — Mask R-CNN (ResNet50-FPN)\n",
    "\n",
    "`torchvision.models.detection.maskrcnn_resnet50_fpn` can produce per-instance masks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c90cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "model_mask = maskrcnn_resnet50_fpn(pretrained=True).to(device).eval()\n",
    "# Run same small inference demo as detection\n",
    "with torch.no_grad():\n",
    "    preds = model_mask(inputs)\n",
    "for i,p in enumerate(preds):\n",
    "    print(f'Image {i}: {len(p[\"masks\"])} masks, boxes: {len(p[\"boxes\"])}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89de73d6",
   "metadata": {},
   "source": [
    "\n",
    "## Semantic Segmentation — DeepLabV3 (ResNet50 backbone)\n",
    "\n",
    "Use `torchvision.models.segmentation.deeplabv3_resnet50`. Example uses VOC or resized CIFAR images for quick inference demo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c562441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "model_seg = deeplabv3_resnet50(pretrained=True).to(device).eval()\n",
    "\n",
    "# run inference on a resized batch\n",
    "inputs_seg = torch.stack([T.ToTensor()(resize(img, (224,224))) for img in batch]).to(device)\n",
    "with torch.no_grad():\n",
    "    out = model_seg(inputs_seg)['out']  # (N, C, H, W)\n",
    "    pred_mask = out.argmax(1)\n",
    "print('Segmentation output shape:', out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83df9bae",
   "metadata": {},
   "source": [
    "\n",
    "## Keypoint Detection — Keypoint R-CNN (pose estimation)\n",
    "\n",
    "`torchvision.models.detection.keypointrcnn_resnet50_fpn` estimates COCO-style keypoints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391403a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchvision.models.detection import keypointrcnn_resnet50_fpn\n",
    "model_kp = keypointrcnn_resnet50_fpn(pretrained=True).to(device).eval()\n",
    "with torch.no_grad():\n",
    "    kp_preds = model_kp(inputs)\n",
    "for i,p in enumerate(kp_preds):\n",
    "    print(f'Image {i}: keypoints tensor shape for first instance (if any):', p.get('keypoints').shape if len(p.get('keypoints',[]))>0 else 'none')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7188d64d",
   "metadata": {},
   "source": [
    "\n",
    "## Super-Resolution — SRCNN (simple example)\n",
    "\n",
    "A small SRCNN-like model to demonstrate upsampling and training for super-resolution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397996dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Small SRCNN-like model\n",
    "class SRCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3,64,9,padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64,32,5,padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32,3,5,padding=2)\n",
    "        )\n",
    "    def forward(self,x): return self.net(x)\n",
    "\n",
    "# demo: take CIFAR images, create low-res by downsampling and upsampling back\n",
    "import torch.nn.functional as F\n",
    "model_sr = SRCNN().to(device)\n",
    "imgs = images[:4].to(device)\n",
    "low = F.interpolate(imgs, scale_factor=0.5, mode='bilinear', align_corners=False)\n",
    "low_up = F.interpolate(low, scale_factor=2.0, mode='bilinear', align_corners=False)\n",
    "with torch.no_grad():\n",
    "    out = model_sr(low_up)\n",
    "print('SR output shape:', out.shape)\n",
    "imshow_batch(out.cpu(), n=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d20d1c",
   "metadata": {},
   "source": [
    "\n",
    "## Generative Models — DCGAN (MNIST demo)\n",
    "\n",
    "Simple DCGAN generator + discriminator and a training loop for MNIST. Provided as a runnable example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d7df86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DCGAN simple implementation (MNIST)\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "\n",
    "# Data\n",
    "mnist_tfms = T.Compose([T.ToTensor(), T.Normalize((0.5,),(0.5,))])\n",
    "mnist = datasets.MNIST('./data', download=True, train=True, transform=mnist_tfms)\n",
    "mn_loader = DataLoader(mnist, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "# Models (small DCGAN)\n",
    "class DCGAN_G(nn.Module):\n",
    "    def __init__(self, zdim=100):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(zdim, 128, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(128), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 1, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self,z): return self.net(z)\n",
    "\n",
    "class DCGAN_D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1,64,4,2,1), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64,128,4,2,1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Flatten(), nn.Linear(128*7*7,1), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self,x): return self.net(x)\n",
    "\n",
    "G = DCGAN_G().to(device)\n",
    "D = DCGAN_D().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optG = optim.Adam(G.parameters(), lr=2e-4, betas=(0.5,0.999))\n",
    "optD = optim.Adam(D.parameters(), lr=2e-4, betas=(0.5,0.999))\n",
    "\n",
    "# Smoke-test: generate samples\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(16,100,1,1, device=device)\n",
    "    samples = G(z).cpu()\n",
    "    # normalize to [0,1] for plotting\n",
    "    samples = (samples + 1) / 2\n",
    "    fig, axes = plt.subplots(1,8, figsize=(12,2))\n",
    "    for i in range(8):\n",
    "        axes[i].imshow(samples[i,0].numpy(), cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e081db23",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Notes and tips\n",
    "- For detection/segmentation/keypoint tasks you usually need COCO or Pascal VOC datasets. These are large; for quick experiments prefer small datasets like PennFudan for instance segmentation (see torchvision tutorials).\n",
    "- Many models above are pretrained and intended for inference or fine-tuning. Fine-tuning requires adjusting the head (box predictor, mask predictor, classifier layer) for the desired number of classes.\n",
    "- If you want I can: add ready-made dataset adapters (PennFudan loader), add full training loops for detection/segmentation, or include automated finetune recipes.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
